{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial 1: SAE Basics - The Prism for Language Models\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand what Sparse Autoencoders (SAEs) are and why they matter\n",
        "- Load a pre-trained SAE and language model\n",
        "- Extract feature activations from text\n",
        "- Decode features to understand what they represent\n",
        "\n",
        "**Estimated Time:** 10-15 minutes\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is a Sparse Autoencoder?\n",
        "\n",
        "Imagine a language model as a complex machine that processes text. Inside this machine, information flows through many layers, but we can't easily see *what* the model is thinking about at each step.\n",
        "\n",
        "A **Sparse Autoencoder (SAE)** acts like a **prism** that breaks down the model's internal representations into interpretable features. Just as a prism splits white light into distinct colors, an SAE decomposes the model's activations into meaningful components.\n",
        "\n",
        "### The Prism Metaphor\n",
        "\n",
        "```\n",
        "Text Input → Language Model → Dense Activations → SAE → Sparse Features\n",
        "   \"Paris\"        (Gemma-2)      [2048 numbers]    (Prism)   [16k features]\n",
        "                                                              Feature #1234: \"French cities\"\n",
        "                                                              Feature #5678: \"European capitals\"\n",
        "                                                              Feature #9012: \"Tourist destinations\"\n",
        "```\n",
        "\n",
        "Each feature represents a specific concept or pattern the model has learned. By analyzing which features activate, we can understand what the model is \"thinking.\"\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Import Libraries\n",
        "\n",
        "We'll use:\n",
        "- `torch`: For tensor operations\n",
        "- `transformer_lens`: For loading language models with easy activation access\n",
        "- `sae_lens`: For loading pre-trained SAEs\n",
        "- `matplotlib`: For visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
      ],
      "source": [
        "import torch\n",
        "from transformer_lens import HookedTransformer\n",
        "from sae_lens import SAE\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Model and SAE\n",
        "\n",
        "We'll use:\n",
        "- **Model:** Gemma-2-2b (a 2-billion parameter language model)\n",
        "- **SAE:** GemmaScope layer 5, 16k features (trained on Gemma-2's activations)\n",
        "\n",
        "This will download models (~5GB) on first run. Subsequent runs will use cached versions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "\n",
            "Loading SAE...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Gemma-2-2b model...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model gemma-2-2b into HookedTransformer\n",
            "\n",
            "✓ Models loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Auto-detect device (Apple Silicon MPS, CUDA, or CPU)\n",
        "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load SAE\n",
        "print(\"\\nLoading SAE...\")\n",
        "sae = SAE.from_pretrained(\n",
        "    release=\"gemma-scope-2b-pt-res-canonical\",\n",
        "    sae_id=\"layer_5/width_16k/canonical\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Load language model\n",
        "print(\"Loading Gemma-2-2b model...\")\n",
        "model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
        "\n",
        "print(\"\\n✓ Models loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Features from Simple Text\n",
        "\n",
        "Let's analyze a simple sentence: **\"The cat sat on the mat\"**\n",
        "\n",
        "We'll:\n",
        "1. Run the text through the model\n",
        "2. Extract activations from layer 5\n",
        "3. Apply the SAE to get sparse features\n",
        "4. See which features activate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: 'The cat sat on the mat'\n",
            "Tokens: ['<bos>', 'The', ' cat', ' sat', ' on', ' the', ' mat']\n",
            "\n",
            "Activation shape: torch.Size([2304])\n",
            "This is a dense vector with 2304 dimensions\n",
            "\n",
            "Feature activations shape: torch.Size([16384])\n",
            "The SAE has 16384 features\n",
            "\n",
            "✓ Found 91 active features\n",
            "  Total activation energy: 406.055\n",
            "\n",
            "Top 3 Active Features:\n",
            "1. Feature #14203 (magnitude: 29.506)\n",
            "   Promotes:  mat, mat,  Mat, Mat,  MAT\n",
            "2. Feature #11477 (magnitude: 17.740)\n",
            "   Promotes:  kneeling,  kneel,  knees,  knelt,  prostrate\n",
            "3. Feature #697 (magnitude: 14.543)\n",
            "   Promotes: ',  , ’,  […], \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Simple example text\n",
        "text = \"The cat sat on the mat\"\n",
        "\n",
        "# Step 1: Convert text to tokens\n",
        "tokens = model.to_tokens(text)\n",
        "print(f\"Text: '{text}'\")\n",
        "print(f\"Tokens: {model.to_str_tokens(tokens[0])}\")\n",
        "\n",
        "# Step 2: Run model and get activations\n",
        "_, cache = model.run_with_cache(tokens)\n",
        "activations = cache[\"blocks.5.hook_resid_post\"][0, -1, :]  # Last token's activation\n",
        "\n",
        "print(f\"\\nActivation shape: {activations.shape}\")\n",
        "print(f\"This is a dense vector with {activations.shape[0]} dimensions\")\n",
        "\n",
        "# Step 3: Apply SAE (the prism!)\n",
        "activations_2d = activations.unsqueeze(0)  # Add batch dimension\n",
        "feature_acts = sae.encode(activations_2d).squeeze()\n",
        "\n",
        "print(f\"\\nFeature activations shape: {feature_acts.shape}\")\n",
        "print(f\"The SAE has {feature_acts.shape[0]} features\")\n",
        "\n",
        "# Step 4: Filter for active features (magnitude > 0)\n",
        "active_mask = feature_acts > 0\n",
        "active_indices = torch.nonzero(active_mask).squeeze()\n",
        "active_magnitudes = feature_acts[active_indices]\n",
        "\n",
        "print(f\"\\n✓ Found {len(active_indices)} active features\")\n",
        "print(f\"  Total activation energy: {active_magnitudes.sum().item():.3f}\")\n",
        "\n",
        "\n",
        "# Decode top 3 features\n",
        "def decode_feature(feature_id, top_k=5):\n",
        "    feature_direction = sae.W_dec[feature_id]\n",
        "    logits = model.unembed(feature_direction)\n",
        "    top_token_ids = logits.argsort(descending=True)[:top_k]\n",
        "    top_words = model.to_str_tokens(top_token_ids)\n",
        "    return top_words\n",
        "\n",
        "top_3_indices = active_indices[torch.argsort(active_magnitudes, descending=True)[:3]]\n",
        "print(\"\\nTop 3 Active Features:\")\n",
        "for i, feat_id in enumerate(top_3_indices.tolist(), 1):\n",
        "    magnitude = feature_acts[feat_id].item()\n",
        "    words = decode_feature(feat_id, top_k=5)\n",
        "    print(f\"{i}. Feature #{feat_id} (magnitude: {magnitude:.3f})\")\n",
        "    print(f\"   Promotes: {', '.join(words)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "In this tutorial, you learned:\n",
        "\n",
        "1. **SAEs as Prisms:** SAEs decompose dense model activations into interpretable sparse features\n",
        "2. **Feature Extraction:** How to run text through a model and extract SAE features\n",
        "3. **Feature Decoding:** How to interpret features by projecting them onto the vocabulary\n",
        "4. **Sparsity:** Only a small fraction of features activate for any given text (~100-300 out of 16,384)\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "In **Tutorial 2: Feature Extraction**, we'll:\n",
        "- Use the reusable functions from `hallucination_detector` package\n",
        "- Compare features between different texts\n",
        "- Identify unique and shared features\n",
        "- Set the foundation for hallucination detection\n",
        "\n",
        "---\n",
        "\n",
        "### Further Exploration\n",
        "\n",
        "Try modifying the `text` variable above with different sentences:\n",
        "- \"Paris is the capital of France\"\n",
        "- \"The dog barked loudly\"\n",
        "- \"Machine learning is fascinating\"\n",
        "\n",
        "Observe how different texts activate different features!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
